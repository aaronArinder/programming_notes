
9: UNDERSTANDING EXPERIMENTAL DATA

- overview
  - mixing computational analysis with social/physical experiments

- big idea
  - we can study how well our models fit the observed data by grpahing our model, the observed
    data, and seeing how far (and how many) data points fall off that graph

- details
  - example is measuring the displacement of a spring; that is, how much force is required
    for a spring to move a certain distance
    - we'd expect a spring to have a constant value that determines how much force, for some
      string, is needed to move it some distance d
  - we can fit a curve  to the graph that represents our expectation of the data (i.e., our
    computational model)
  - when data points aren't on that curve, i.e. when they don't meet our model's expectation, we
    can measure their distance, and since the x-axis represents independent variables and the
    y-axis represents dependent variables, we can measure the effect of those dependent variables
    - we measure their distance by taking the difference between the observed data and the predicted
      data, and squaring it. This is called the LEAST SQUARES objective function.
      - this is a function because we look at each data point, both predicted and actual, and the
        distance between them represents the degree to which our model lacks fit with the actual data
      - we square to remove sign
      - why not use absolute value?
        - least squares / number of observations = its variance
          - we want to minimize variance; the less variance, the better the fit between our model
            and the observed data

  - we want to decrease variance, and we can do that with linear regression
    - if we assume our model is a polynomial in one variable, we can solve for the minimum value of
      least squares and thereby get our variance down and have a better fit between the
      predicted/actual data
      - simple example for a line: y = ax+b, where a is our constant, x is the x-axis value for each
        datum, and b is an other modifier/interaction present.
          - e.g., a polynomial describing the total area of a window when given only the glass portion,
          x, looks something like this: (general area taken up by glass)(units of glass) + (area added
          by other elements, such as trim).
      - pyFill is what the course uses to find the best fit

  - we can determine the goodness of any fit, no matter how we model the data (i.e., whether
    quadratically, linearally, and so on)
    - coefficient of determination: 1 - (sum(error in estimates) / sum(variability in measured data))
      - both sides of the inequality are squared to avoid negatives
      - the error in estimates is had by the difference between measured and predicated values
        - measured - predicted
      - the variability in measured data is had from the difference between the predicted values
        and the mean of measured values
        - measured values - mean of measured values
      - when the coefficient of determin is 1, we've no variability; when 0, model totally at odds
        with measured data


10: UNDERSTANDING EXPERIMENTAL DATA (cont.)
  - overview

  - big idea

  - details
    - when we increase the order of variability past a certain threshold we can better fit the measured
    data, but we will struggle to predict new data.
    - what if we try to fit a higher degree of variablity model to something like a straight line?
      Since the coefficient will be 0, it shouldn't matter, right?
        - Wrong: with a small amount of data noise, that higher degree of variability will mushroom
          and make it so that your model is off by a big percentage.
            - Better to use a lower-order degree of variability, which can accomodate data noise
              since it doesn't grow nearly as quickly as a higher-degree order of variability.
    - we can break up the data into different segments to find different models for the best fit
      for each segment
    - cross validation

11: INTRODUCTION TO MACHINE LEARNING
  - overview

  - big idea
    - we get better clustering of data points when we use unlabeled data (i.e., do machine learning
      without supervision) because the most natural grouping can rise to the top rather than conform
      to our expectations

  - details
    - What is machine learning?
      - it's a modifcation of non-learning programming. We go from data + program = output to
        data + desired output = program.
          - notice something interesting, when we combine machine and non-machine-learning programs:
            (data + desired output) + data = output
              - this is the iteration used in machine learning
    - machines generalize rather than memorize data
    - supervised training labels each bit of data
    - unsupervised training doesn't label each bit of data, but lets the machine try to group
      them sensibly by some pattern
    - it's important to `feature engineer` in a way that allows for the easy generalization of
      the properties of interest
    - how to measure accuracy: take the confusion matrix,
        true pos  | false pos
        false neg | true neg
      and find the accuracy: true pos + true neg / true pos + true neg + false pos + false neg
        - this says: total number of correct answers / total answers, which gives correct percentage
    - two other measures of accuracy, sensitivity, which gives you percentage gotten correct, and
      specificity, which is the percentage incorrect
        - sensitivity: true pos / true pos + false neg
        - specificity: true neg / true neg + false pos

12: CLUSTERING
  - overview
  - big idea
  - details
    - clustering is an optimization problem
    - variability of a single cluster: sum of the distance between the mean of the cluster and
      the distance of each example
    - dissimilarity of a group of cluster: sum of all the individual variabilitiesA
    - dividing the variability by the size of cluster gives variance; but, we don't want that
      because doing so normalizes that number.
        - the problem with normalizing it is that the `penalty` (wut?) of a big cluster with a lot
          of variability is the same as the `penalty` (again, wut) of a smaller cluster with a lot
          of variability.
            - the idea is that you lose the size and number of variabilities, I suppose; you end up
              with just one number, the variance, for both large and small clusters, without any
              sense of how many different points varied
                - 'big and bad is worse than small and bad'
    - can we understand the optimization problem of clustering as finding a group of clusters that
      minimizes dissimilarity?
        - no: we can cook just let each point on the graph be its own cluster, which minimizes
          the distance of each group of clusters (since each group has one member), but that's
          obviously bad
        - we can use constraints, though, on the clusters to get around the immediately above
          - then treat it as optimization problem
    - hierarchical clustering: start by assigning each item to a cluster, so that if you have
      N items, you now have N clusters, each containing just one item; find the closest (most
      similar) pair of clusters, merge them into one cluster; repeat until all clusters are in
      one cluster with N members.
        - kind of dumb, but interesting because we normally stop before hitting just one cluster.
        - we get a dendogram (shows us what we've merged thus far) when we smoosh clusters
        - this hierarchical clustering is called conglomerative
        - catch: what do mean by distance? lots of notions of it
          - maybe closet points or farthest points in two clusters
    - we could cluster with a greedy algorithm
    - we could cluster with k-means algorithm
      - randomly chose some number of examples as initial centroids (a randomly selected point
        that isn't a data point but acts as the center point for the cluster); create k number of
        clusters by assigning each example to the closest centroid; compute k new centroids by
        averaging the examples in each centroid; if the centroids don't change, then end
          - this is pretty fast: there are k numbers of clusters, and for each example it'll need
            to be compared with each centroied: so, k times n comparison times times the time it
            takes to make the comparison
          - how do we choose k centroids? often a priorily: prior knowledge helps, but we can
            also search for a good k; we can also run hierachical clustering to determine it
          - we can also check randomly chosen centroids and find the best one by checking how
            dissimilar each are (and keeping track of the best one along the way)

13: CLASSIFICATION
  - overview
  - big idea
  - details
    - 12 deals with unsupervised learning, this with supervised
    - classification can be roughly divided into two categories
      - regression: predicting real numbers associated with a feature
      - classification: predicting a discrete value (label) associated with a feature
    - nearest neighbor: label x as L if x is closer to y than z, otherwise, label it whatever's
      z's label is
    - k-nearest neighbors: cluster the nearest neighbors, and label x whatever label is the majority
      - better than simple nearest neighbor because it quietens down outliers
      - have to determine the size of k, just like in lesson 12
      - memory intensive for bigger datasets, meaning it can take a while
    - sensitivity, specificity, positive predictive value, and negative predictive value come from
      lesson 11
    - logistic regression: finds weights for each feature. Unclear what, exactly, it does.


14: CLASSIFICATION AND STATISTICAL SINS
  - overview
  - big idea
  - details
    - features are often correlated, so each feature's weight can't be evaluated on its own
    - two major uses of logistic regression: L1 and L2
      - L1: find some weights and drives them to 0; takes a big dataset and gives 0 weights to a bunch
        of features; big goal is to avoid overfitting
          - problem: if two features correlated, driving one of them to 0 will mask that correlation
      - L2: spreads weight across variables: if correlted variables, might look like none are important
        because they each get a small amount of the weight. Smooths out the correlation.
      - the probability parameter, P, is the number we use to make predictions; e.g., if P = .5, we say
        that those with P > .5 have some prediction, p, and those under, have not-p.
        - Receiver operating characteristic: if we don't want to set a cutoff, we can look at the 'shape'
          of all possible cutoffs to see where our predictions would lie given different Ps
    - Receiver Operating Characteristic:


A big idea: trading time and space
